{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics collection architecture\n",
    "\n",
    "\n",
    "Instead of reinventing this, we could either use `snowplow` altogether or build a similar pipeline.\n",
    "This takes care of everything related to collecting, validating, enriching (or anonymizing) and ultimately \n",
    "\"hosing\" the data into cloud storage to be consumed by the model-trainer.\n",
    "![](collector-architecture-snowplow.png)\n",
    "> From https://docs.snowplow.io/docs/understanding-your-pipeline/architecture-overview/\n",
    "\n",
    "\n",
    "# Interaction diagram of overall architecture\n",
    "\n",
    "![](recommender_arch.drawio.png)\n",
    "\n",
    "## Obvservations\n",
    "\n",
    "* Why not lambda function?\n",
    "  * Realtime requirement doesn't mix well with lambda cold starts\n",
    "* Why Redis?\n",
    "  * We don't have a lot of requirements for the data store here, but Redis _could_ provide us with realtime queries â€“ HOWEVER, we would probably be better off caching the ML-model in the memory of the REST containers themselves. This would also lessen the dependence on any particular data store.\n",
    "  * Note that the current model weighs in at about `~14mb`, so so it really should reside as close to the REST containers as possible, even if we could transfer the model realtively fast on each request.\n",
    "* Why AWS?\n",
    "  * It doesn't have to be. As long as our ML-pipeline AND our analytics provider supports the cloud storage service of said cloud provider.\n",
    "* Is the ML-pipeline realtime\n",
    "  * It absolutely doesn't have to be. There might be special cases where new products are introduced and we need to make sure they are included as soon as possible, but we will need special care for that scenario anyway. The pipeline could run in batch mode on a regular basis.\n",
    "  * Updating an existing model in realtime might be more error-prone than creating fresh models with a new window every-so-often. It all comes down to cost and business requirements.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
